
# Run m-slam all the way through to splatting
# Starting with the soneva reef dataset one, left hand side (LHS) only first.

# First had to downsample to 1600. Used:
/home/bwilliams/encode/code/dev/downsample_img.sh # change dirs in the script for future use

# Then have to convert to png:
conda activate mast3r-slam
python /home/bwilliams/encode/code/dev/jpeg2png.py # again, change dirs as appropriate

# Now try mast3r-slam:
python main.py --dataset datasets/reef_soneva --config config/base.yaml

# Some notes:
- It seems to sort of work. There are some gaps and I wonder if these are because its only using mono not stereo?
  so how to add in second camera?
- Will try to make splat from it, but also don't have the groundtruth from agisoft to compare against like we did
  with the small patch from sweet-coral

# Next I solved the intrinsics estimation. More notes in Notion, but in short we run colmap on the first 100 raw
# images (not keyframes) which can be used to estimate the intrinsics with a bash script that calls colmap functions.
# This will also output a summary text file on how well it did:
cd /home/bwilliams/encode/code
source ben-splat-env/bin/activate
python /home/bwilliams/encode/code/dev/estimate_intrinsics.py \
--images_path /home/bwilliams/encode/data/soneva/ootbm/LHS_downsampled_png/ \
--dataset reef_soneva \
--num_images 100 \
--camera_model OPENCV

# Then we take the outputs and create an intrinsics.yaml that can be used by master-slam to improve the slam. 
# It also will convert the intrinsics to cameras.bin/txt files for lichtfeld-studio splatting.
# Key: intrinsics.yaml uses OPENCV (with distortion) at raw resolution for MASt3R-SLAM.
#      cameras.bin/txt use PINHOLE (no distortion) at SLAM resolution for LichtFeld-Studio.
#      This is because MASt3R-SLAM undistorts keyframes internally before saving them.
conda activate mast3r-slam
python /home/bwilliams/encode/code/dev/shuttle_intrinsics.py --dataset reef_soneva

# Next, we can actually run m-slam. We will find the reconstruction is better with these intrinisics than with
# the default intrinsics yaml used by m-slam:
cd /home/bwilliams/encode/code/MASt3R-SLAM
python /home/bwilliams/encode/code/MASt3R-SLAM/main.py \
 --dataset /home/bwilliams/encode/data/soneva/ootbm/LHS_downsampled_png \
 --config /home/bwilliams/encode/code/MASt3R-SLAM/config/base.yaml \
 --calib /home/bwilliams/encode/data/intermediate_data/reef_soneva/intrinsics.yaml

# Getting a strange `FileNotFoundError: [Errno 2] No such file or directory` error? For some reason
# it seems trying to run on reef data as the first dataset after reboot doesn't work. So first run the 
# demo data, then try the above command to run the reef data again.

# Now master-slam has run we want to convert the poses estimated by m-slam to the colmap format in images.bin.
# We'll put this converted file into the splatting folder and copy over the keyframes from m-slam as well
python /home/bwilliams/encode/code/dev/cam_pose_keyframes_shuttle.py --dataset reef_soneva

# Now convert the .ply pointcloud output by m-slam to a points3d.bin for lichtfeld-studio. This will read where the
# .ply is and save the new points3d.bin alongside the cameras.bin and images.bin file ready for splatting.
/home/bwilliams/encode/code/dev/mslam_ply_to_points3d.py --dataset reef_soneva

# Now lets try splatting with the m-slam outputs and conversions to colmap:
python /home/bwilliams/encode/code/dev/train_splat.py \
  --lichtfeld /home/bwilliams/encode/code/lichtfeld-studio/build/LichtFeld-Studio \
  -d /home/bwilliams/encode/data/intermediate_data/reef_soneva/for_splat \
  -o /home/bwilliams/encode/code/lichtfeld-studio/output/scratch \
  --headless -i 250000 --max-cap 1000000

 




############################
# Scratch stuff
# rerun.io
# Now we can visualize the SLAM outputs to verify the camera poses and point cloud look correct:
source ben-splat-env/bin/activate
python /home/bwilliams/encode/code/scratch/visualize_slam_rerun.py --dataset reef_soneva

# Optional flags:
# --no-images              # Skip loading keyframe images (faster, shows only frustums)
# --downsample 0.1         # Show only 10% of points (useful for large point clouds)

# What to check in Rerun viewer:
# - Camera frustums positioned around the point cloud
# - Cameras "look into" the scene (not away from it)
# - Scale is consistent (cameras not too huge/tiny)
# - Use timeline slider to step through camera sequence
# - Click on frustums to see the actual keyframe images

