
# Run m-slam all the way through to splatting
# Starting with the soneva reef dataset one, left hand side (LHS) only first.

# First had to downsample to 1600. Used:
/home/bwilliams/encode/code/dev/downsample_img.sh # change dirs in the script for future use

# Then have to convert to png:
conda activate mast3r-slam
python /home/bwilliams/encode/code/dev/jpeg2png.py # again, change dirs as appropriate

################################################################################
# FULL PIPELINE (Automated via run_pipeline.py)
################################################################################

# The full pipeline from intrinsics estimation through gaussian splatting is now
# automated with run_pipeline.py. This ensures all outputs are organized in one
# directory and the configuration is saved for reproducibility.

# Step 1: Create a config file (copy from template)
cp /home/bwilliams/encode/code/dev/pipeline_config_template.yaml my_run_config.yaml
# Edit my_run_config.yaml:
#   - Set run_name (e.g., "reef_soneva")
#   - Set images_path
#   - Adjust parameters as needed

# Step 2: Run the complete pipeline
cd /home/bwilliams/encode/code
conda activate mast3r-slam  # or ben-splat-env
python /home/bwilliams/encode/code/dev/run_pipeline.py --config my_run_config.yaml

# The pipeline will:
# 1. Estimate intrinsics with COLMAP
# 2. Convert intrinsics for MASt3R-SLAM and LichtFeld
# 3. Run MASt3R-SLAM
# 4. Move MASt3R-SLAM outputs to run directory
# 5. Convert poses to COLMAP format
# 6. Convert PLY to points3D.bin
# 7. Train Gaussian Splatting

# All outputs will be in:
# /home/bwilliams/encode/data/intermediate_data/{run_name}/
#   ├── pipeline_config.yaml     # Config used for this run
#   ├── pipeline.log             # Full execution log
#   ├── colmap_outputs/          # COLMAP intrinsics
#   ├── intrinsics.yaml          # For MASt3R-SLAM
#   ├── mslam_logs/              # MASt3R-SLAM outputs (keyframes, poses, PLY)
#   ├── for_splat/               # COLMAP format for splatting
#   └── splats/                  # Final splat outputs

# To resume from a specific step (e.g., if step 3 failed):
python /home/bwilliams/encode/code/dev/run_pipeline.py --config my_run_config.yaml --start-from 3

# To run only a specific step (e.g., re-run splatting with different params):
python /home/bwilliams/encode/code/dev/run_pipeline.py --config my_run_config.yaml --only 7
# Note: Re-running step 7 will create splats1/, splats2/, etc. to avoid overwriting previous results

# To experiment with different splatting parameters without editing config:
python /home/bwilliams/encode/code/dev/run_pipeline.py --config my_run_config.yaml --only 7 --max-cap 500000
python /home/bwilliams/encode/code/dev/run_pipeline.py --config my_run_config.yaml --only 7 -i 50000 --max-cap 2000000

################################################################################
# MANUAL STEP-BY-STEP (Original Method - Still Supported)
################################################################################

# If you prefer to run steps manually or need more control, you can still run
# each script individually:
cd /home/bwilliams/encode/code
source ben-splat-env/bin/activate
python /home/bwilliams/encode/code/dev/estimate_intrinsics.py \
--images_path /home/bwilliams/encode/data/soneva/ootbm/LHS_downsampled_png/ \
--dataset reef_soneva \
--num_images 100 \
--camera_model OPENCV

# Then we take the outputs and create an intrinsics.yaml that can be used by master-slam to improve the slam. 
# It also will convert the intrinsics to cameras.bin/txt files for lichtfeld-studio splatting.
# Key: intrinsics.yaml uses OPENCV (with distortion) at raw resolution for MASt3R-SLAM.
#      cameras.bin/txt use PINHOLE (no distortion) at SLAM resolution for LichtFeld-Studio.
#      This is because MASt3R-SLAM undistorts keyframes internally before saving them.
conda activate mast3r-slam
python /home/bwilliams/encode/code/dev/shuttle_intrinsics.py --dataset reef_soneva

# Next, we can actually run m-slam. We will find the reconstruction is better with these intrinisics than with
# the default intrinsics yaml used by m-slam:
cd /home/bwilliams/encode/code/MASt3R-SLAM
python /home/bwilliams/encode/code/MASt3R-SLAM/main.py \
 --dataset /home/bwilliams/encode/data/soneva/ootbm/LHS_downsampled_png \
 --config /home/bwilliams/encode/code/MASt3R-SLAM/config/base.yaml \
 --calib /home/bwilliams/encode/data/intermediate_data/reef_soneva/intrinsics.yaml

# Getting a strange `FileNotFoundError: [Errno 2] No such file or directory` error? For some reason
# it seems trying to run on reef data as the first dataset after reboot doesn't work. So first run the 
# demo data, then try the above command to run the reef data again.

# Now master-slam has run we want to convert the poses estimated by m-slam to the colmap format in images.bin.
# We'll put this converted file into the splatting folder and copy over the keyframes from m-slam as well
python /home/bwilliams/encode/code/dev/cam_pose_keyframes_shuttle.py --dataset reef_soneva

# Now convert the .ply pointcloud output by m-slam to a points3d.bin for lichtfeld-studio. This will read where the
# .ply is and save the new points3d.bin alongside the cameras.bin and images.bin file ready for splatting.
/home/bwilliams/encode/code/dev/mslam_ply_to_points3d.py --dataset reef_soneva

# Now lets try splatting with the m-slam outputs and conversions to colmap:
python /home/bwilliams/encode/code/dev/train_splat.py \
  --lichtfeld /home/bwilliams/encode/code/lichtfeld-studio/build/LichtFeld-Studio \
  -d /home/bwilliams/encode/data/intermediate_data/reef_soneva/for_splat \
  -o /home/bwilliams/encode/code/lichtfeld-studio/output/scratch \
  --headless -i 250000 --max-cap 1000000

 




############################
# Scratch stuff
# rerun.io
# Now we can visualize the SLAM outputs to verify the camera poses and point cloud look correct:
source ben-splat-env/bin/activate
python /home/bwilliams/encode/code/scratch/visualize_slam_rerun.py --dataset reef_soneva

# Optional flags:
# --no-images              # Skip loading keyframe images (faster, shows only frustums)
# --downsample 0.1         # Show only 10% of points (useful for large point clouds)

# What to check in Rerun viewer:
# - Camera frustums positioned around the point cloud
# - Cameras "look into" the scene (not away from it)
# - Scale is consistent (cameras not too huge/tiny)
# - Use timeline slider to step through camera sequence
# - Click on frustums to see the actual keyframe images

